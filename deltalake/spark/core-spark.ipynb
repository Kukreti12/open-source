{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce\n",
    "![img](maprreduce.png)\n",
    "\n",
    "\n",
    "Why not map reduce\n",
    "1. Not designed for followings\n",
    "    1. Interactive queries\n",
    "    2. Iterative\n",
    "    3. Low latency like streaming\n",
    "    4. Map output is write to the local disk and the reduce output is write to the HDFS\n",
    "2. Need different tooling to implement\n",
    "    1. SQL(Hive)\n",
    "    2. Machine learning(Mahout)\n",
    "    3. Graph processing(Giraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark\n",
    "1. Compute engine\n",
    "2. Unified data processing\n",
    "3. Strong consistent API\n",
    "4. Low disk I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "Resilient distributed datasets\n",
    "1. Each partition is processed in parallel and executed by the executors. In case of failure only the portion of data will be processed again from the source data. \n",
    "2. It maintain the lineage of transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "1. Spark Context\n",
    "    1. Main entry point of spark functionality\n",
    "    2. sparkcontext()- load the default functionality of spark but we can configure and override the sc by defining arguments in sc().\n",
    "2. ClusterManager\n",
    "    1. Standalone\n",
    "    2. Apache Mesos\n",
    "    3. Hadoop yarn\n",
    "    4. Kubernetes- An open source system for automating deployment, scaling and management of containerized application\n",
    "3. Spark Architecture\n",
    "    1. Executor is like service run on worker node. Multiple executors can run on single machine. Each executors are allocated resource by the cluster manager\n",
    "    2. SparkContext on the driver node  can be connected to several type of cluster manager.\n",
    "![spark](spark-arch.PNG)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 2916, 100]\n",
      "54\n",
      "[2, 54, 10]\n"
     ]
    }
   ],
   "source": [
    "## Python\n",
    "list1=[1,2,3,54,10]\n",
    "\n",
    "## define square function\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "map_result =list(map(square,list1))\n",
    "print(map_result)\n",
    "\n",
    "## Reduce: it return the single value instead of the list like map and the filter methods\n",
    "from functools import reduce\n",
    "\n",
    "reduce_output= reduce(lambda x,y: x if x>y else y, list1)\n",
    "print(reduce_output)\n",
    "\n",
    "#filter\n",
    "filter_output=list(filter(lambda x: x %2==0,list1))\n",
    "print(filter_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD from list\n",
    "list2=[2,5,8,9,0,1]\n",
    "rdd=sc.parallelize(list2,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[4, 25, 64, 81, 0, 1]\n",
      "9\n",
      "[2, 8, 0]\n"
     ]
    }
   ],
   "source": [
    "##map\n",
    "map_rdd =rdd.map(lambda x:x**2)\n",
    "print(map_rdd.getNumPartitions())\n",
    "print(map_rdd.collect())\n",
    "#reduce\n",
    "reduce_rdd=rdd.reduce(lambda x,y:x if x>y else y)\n",
    "print(reduce_rdd)\n",
    "#filter\n",
    "even_rdd =rdd.filter(lambda x:x%2==0).collect()\n",
    "print(even_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Aggregation by reducebykey\n",
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Transformation   \n",
    "    1. narrow: where partition is based on 1 on 1 mapping. If we pass the number of partition in arguments. Then the same number of partition is created\n",
    "    2. wide: Where the spark do the shuffling of all the partitions which can be very cumbersome when we do for the large data.\n",
    "2. Job: whenever we call the below actions then the spark jobs will be created.\n",
    "    1. Collect\n",
    "    2. take\n",
    "    3. save as text file\n",
    "3. Stages: The number of stages depend on the wide transformation. All the narrow transformation will be compacted to 1 stage. Total number of stages will always be wide transformation plus 1.\n",
    "4. Tasks: depend on the number of partitions. For every partition there will be one task.Each stage there will be tasks running. For example if we have two partition and there are two stages. then each stage will run 2 tasks for each partition.\n",
    "\n",
    "Concepts around parition\n",
    "1. On raw data when we create RDD and define the partition it wil create the parition based on the number user give. After the action or transformation The data is pushed in the partition based on the hash parition logic where records are sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset location\n",
    "#https://drive.google.com/drive/u/0/folders/1yCNpxbFHyH-AEyVgIQ3gSE_Y-LBZ1DsZ\n",
    "\n",
    "# variable name\n",
    "partition_num=2\n",
    "input_rdd = sc.textFile(\"C:/Users/sharsaur/NA-AI-lakehouse/deltalake/spark/weather.csv\", partition_num)\n",
    "selected_fields_rdd = input_rdd.map(lambda line: (int(line.split(\",\")[0].split(\"-\")[0]), int(line.split(\",\")[2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_temperature_rdd = selected_fields_rdd.reduceByKey(lambda x, y: x if x>y else y)\n",
    "print(\"Max temperature RDD: {}\".format(max_temperature_rdd.collect()))\n",
    "# COMMAND ----------\n",
    "print(\"Partitioner for the max_temperature_rdd is {}\".format(max_temperature_rdd.partitioner.partitionFunc))\n",
    "max_temperature_rdd.saveAsTextFile(r\"C:\\Users\\sharsaur\\NA-AI-lakehouse\\deltalake\\spark\\max_temp\")\n",
    "print(open(r\"C:\\Users\\sharsaur\\NA-AI-lakehouse\\deltalake\\spark\\max_temp\\part-00000\", \"r\").read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### range partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max temperature RDD: [(2016, 36), (2018, 45), (2010, 39), (2014, 35), (2012, 40), (2019, 47), (2017, 47), (2013, 47), (2015, 41), (2011, 38)]\n",
      "Partitioner for the max_temperature_rdd is <function portable_hash at 0x000002345AD53280>\n",
      "Sorted RDD: [(2010, 39), (2011, 38), (2012, 40), (2013, 47), (2014, 35), (2015, 41), (2016, 36), (2017, 47), (2018, 45), (2019, 47)]\n",
      "Partitioner for the sorted_rdd is <function RDD.sortByKey.<locals>.rangePartitioner at 0x000002345D7853A0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sorted_rdd = max_temperature_rdd.sortByKey()\n",
    "print(\"Sorted RDD: {}\".format(sorted_rdd.collect()))\n",
    "print(\"Partitioner for the sorted_rdd is {}\".format(sorted_rdd.partitioner.partitionFunc))\n",
    "sorted_rdd.saveAsTextFile(r\"C:\\Users\\sharsaur\\NA-AI-lakehouse\\deltalake\\spark\\sorted_max_temperature\")\n",
    "print(open(r\"C:\\Users\\sharsaur\\NA-AI-lakehouse\\deltalake\\spark\\sorted_max_temperature\\part-00001\", \"r\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa3b4e107235e815dae340c2aa666ee13c05992348afd4164fcaec1be22e5205"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
