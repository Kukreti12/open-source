{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1795)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:533)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adi3m\\open-source\\deltalake\\deltalake.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adi3m/open-source/deltalake/deltalake.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdelta\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adi3m/open-source/deltalake/deltalake.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m builder \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39msql\u001b[39m.\u001b[39mSparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mMyApp\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adi3m/open-source/deltalake/deltalake.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.extensions\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mio.delta.sql.DeltaSparkSessionExtension\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adi3m/open-source/deltalake/deltalake.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.catalog.spark_catalog\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.sql.delta.catalog.DeltaCatalog\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adi3m/open-source/deltalake/deltalake.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m spark \u001b[39m=\u001b[39m configure_spark_with_delta_pip(builder)\u001b[39m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39mconf \u001b[39mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\context.py:203\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    201\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[0;32m    206\u001b[0m         sparkHome,\n\u001b[0;32m    207\u001b[0m         pyFiles,\n\u001b[0;32m    208\u001b[0m         environment,\n\u001b[0;32m    209\u001b[0m         batchSize,\n\u001b[0;32m    210\u001b[0m         serializer,\n\u001b[0;32m    211\u001b[0m         conf,\n\u001b[0;32m    212\u001b[0m         jsc,\n\u001b[0;32m    213\u001b[0m         profiler_cls,\n\u001b[0;32m    214\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n\u001b[0;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\context.py:296\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf\u001b[39m.\u001b[39m_jconf)\n\u001b[0;32m    297\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.0-bin-hadoop3\\python\\pyspark\\context.py:421\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1588\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, \u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fqn)\n\u001b[0;32m   1590\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-3.5.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1139)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1125)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:489)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1795)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16(SparkContext.scala:533)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$16$adapted(SparkContext.scala:533)\r\n\tat scala.collection.immutable.List.foreach(List.scala:431)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:533)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\\hadoop\\bin\\bin -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:607)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adi3m\\open-source\\deltalake\\deltalake.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adi3m/open-source/deltalake/deltalake.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and write Delta lake tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the credit card data as spark dataframe\n",
    "df=spark.read.option(\"header\",True).csv(\"/opt/spark/python/mldatasets/UCI_Credit_Card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the credit card data as spark dataframe\n",
    "df=spark.read.option(\"header\",True).csv(\"/mnt/deltalake/df7k3s.ailab.local/delta/dbfs/creditcard/mldatasets/UCI_Credit_Card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the directoty \n",
    "!rm -rf /opt/spark/python/cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table as delta lake table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/deltalake/df7k3s.ailab.local/delta/dbfs/creditcard/transactionlog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|ID  |\n",
      "+----+\n",
      "|9999|\n",
      "|9998|\n",
      "|9997|\n",
      "|9996|\n",
      "|9995|\n",
      "|9994|\n",
      "|9993|\n",
      "|9992|\n",
      "|9991|\n",
      "|9990|\n",
      "|999 |\n",
      "|9989|\n",
      "|9988|\n",
      "|9987|\n",
      "|9986|\n",
      "|9985|\n",
      "|9984|\n",
      "|9983|\n",
      "|9982|\n",
      "|9981|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.ID.desc()).select(df[\"ID\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file from delta lake\n",
    "df_delta = spark.read.format(\"delta\").load(\"/opt/spark/python/cc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LIMIT_BAL: string (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- MARRIAGE: string (nullable = true)\n",
      " |-- AGE: string (nullable = true)\n",
      " |-- PAY_0: string (nullable = true)\n",
      " |-- PAY_2: string (nullable = true)\n",
      " |-- PAY_3: string (nullable = true)\n",
      " |-- PAY_4: string (nullable = true)\n",
      " |-- PAY_5: string (nullable = true)\n",
      " |-- PAY_6: string (nullable = true)\n",
      " |-- BILL_AMT1: string (nullable = true)\n",
      " |-- BILL_AMT2: string (nullable = true)\n",
      " |-- BILL_AMT3: string (nullable = true)\n",
      " |-- BILL_AMT4: string (nullable = true)\n",
      " |-- BILL_AMT5: string (nullable = true)\n",
      " |-- BILL_AMT6: string (nullable = true)\n",
      " |-- PAY_AMT1: string (nullable = true)\n",
      " |-- PAY_AMT2: string (nullable = true)\n",
      " |-- PAY_AMT3: string (nullable = true)\n",
      " |-- PAY_AMT4: string (nullable = true)\n",
      " |-- PAY_AMT5: string (nullable = true)\n",
      " |-- PAY_AMT6: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column type from string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta=df_delta.withColumn(\"ID\",col(\"ID\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|ID   |\n",
      "+-----+\n",
      "|30007|\n",
      "|30006|\n",
      "|30005|\n",
      "|30004|\n",
      "|30003|\n",
      "|30002|\n",
      "|30001|\n",
      "|30000|\n",
      "|29999|\n",
      "|29998|\n",
      "|29997|\n",
      "|29996|\n",
      "|29995|\n",
      "|29994|\n",
      "|29993|\n",
      "|29992|\n",
      "|29991|\n",
      "|29990|\n",
      "|29989|\n",
      "|29988|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta.sort(df_delta.ID.desc()).select(df_delta[\"ID\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file as delta lake \n",
    "deltaTable = DeltaTable.forPath(spark, \"/opt/spark/python/cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'delta.tables.DeltaTable'>\n"
     ]
    }
   ],
   "source": [
    "print(type(deltaTable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.update(\n",
    "  condition = col('ID') == '30002',\n",
    "  set = { 'ID': lit(40000) }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete completed\n"
     ]
    }
   ],
   "source": [
    "# Delete 1 row\n",
    "deltaTable=deltaTable.delete(condition = expr(\"ID == 30001\"))\n",
    "# Check the logs \n",
    "print(\"Delete completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the few rows to the existing delta lake tables\n",
    "df_append=spark.read.option(\"header\",True).csv(\"/opt/spark/python/mldatasets/cc-append.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_append.write.format(\"delta\").mode(\"append\").save(\"/opt/spark/python/cc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert using Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge has following features\n",
    "1. Insert new records\n",
    "2. update existing records\n",
    "3. Delete records\n",
    "\n",
    "All above can be done using same statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names\n",
    "ID,LIMIT_BAL,SEX,EDUCATION,MARRIAGE,AGE,PAY_0,PAY_2,PAY_3,PAY_4,PAY_5,PAY_6,BILL_AMT1,BILL_AMT2,BILL_AMT3,BILL_AMT4,BILL_AMT5,BILL_AMT6,PAY_AMT1,PAY_AMT2,PAY_AMT3,PAY_AMT4,PAY_AMT5,PAY_AMT6,default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, '/opt/spark/python/cc')\n",
    "df_merge=spark.read.option(\"header\",True).csv(\"/opt/spark/python/mldatasets/cc-upsert.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LIMIT_BAL: string (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- MARRIAGE: string (nullable = true)\n",
      " |-- AGE: string (nullable = true)\n",
      " |-- PAY_0: string (nullable = true)\n",
      " |-- PAY_2: string (nullable = true)\n",
      " |-- PAY_3: string (nullable = true)\n",
      " |-- PAY_4: string (nullable = true)\n",
      " |-- PAY_5: string (nullable = true)\n",
      " |-- PAY_6: string (nullable = true)\n",
      " |-- BILL_AMT1: string (nullable = true)\n",
      " |-- BILL_AMT2: string (nullable = true)\n",
      " |-- BILL_AMT3: string (nullable = true)\n",
      " |-- BILL_AMT4: string (nullable = true)\n",
      " |-- BILL_AMT5: string (nullable = true)\n",
      " |-- BILL_AMT6: string (nullable = true)\n",
      " |-- PAY_AMT1: string (nullable = true)\n",
      " |-- PAY_AMT2: string (nullable = true)\n",
      " |-- PAY_AMT3: string (nullable = true)\n",
      " |-- PAY_AMT4: string (nullable = true)\n",
      " |-- PAY_AMT5: string (nullable = true)\n",
      " |-- PAY_AMT6: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merge.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias('cc') \\\n",
    "  .merge(df_merge.alias('upsrt'),'cc.ID = upsrt.ID') \\\n",
    "  .whenMatchedUpdate(set ={\\\n",
    "\"ID\": \"upsrt.ID\",\n",
    "\"LIMIT_BAL\":\"upsrt.LIMIT_BAL\",\n",
    "\"SEX\": \"upsrt.SEX\",\n",
    "\"EDUCATION\": \"upsrt.EDUCATION\",\n",
    "\"MARRIAGE\": \"upsrt.MARRIAGE\",\n",
    "\"AGE\": \"upsrt.AGE\",\n",
    "\"PAY_0\": \"upsrt.PAY_0\",\n",
    "\"PAY_2\": \"upsrt.PAY_2\",\n",
    "\"PAY_3\": \"upsrt.PAY_3\",\n",
    "\"PAY_4\":\"upsrt.PAY_4\",\n",
    "\"PAY_5\": \"upsrt.PAY_5\",\n",
    "\"PAY_6\":\"upsrt.PAY_6\",\n",
    "\"BILL_AMT1\": \"upsrt.BILL_AMT1\",\n",
    "\"BILL_AMT2\": \"upsrt.BILL_AMT2\",\n",
    "\"BILL_AMT3\": \"upsrt.BILL_AMT3\",\n",
    "\"BILL_AMT4\": \"upsrt.BILL_AMT4\",\n",
    "\"BILL_AMT5\": \"upsrt.BILL_AMT5\",\n",
    "\"BILL_AMT6\": \"upsrt.BILL_AMT6\",\n",
    "\"PAY_AMT1\": \"upsrt.PAY_AMT1\"  ,\n",
    "\"PAY_AMT2\": \"upsrt.PAY_AMT2\",\n",
    "\"PAY_AMT3\": \"upsrt.PAY_AMT3\",\n",
    "\"PAY_AMT4\": \"upsrt.PAY_AMT4\",\n",
    "\"PAY_AMT5\": \"upsrt.PAY_AMT5\",\n",
    "\"PAY_AMT6\": \"upsrt.PAY_AMT6\",\n",
    "\"default\": \"upsrt.default\"  }) \\\n",
    ".whenNotMatchedInsert(values ={ \\\n",
    "\"ID\": \"upsrt.ID\",\n",
    "\"LIMIT_BAL\":\"upsrt.LIMIT_BAL\",\n",
    "\"SEX\": \"upsrt.SEX\",\n",
    "\"EDUCATION\": \"upsrt.EDUCATION\",\n",
    "\"MARRIAGE\": \"upsrt.MARRIAGE\",\n",
    "\"AGE\": \"upsrt.AGE\",\n",
    "\"PAY_0\": \"upsrt.PAY_0\",\n",
    "\"PAY_2\": \"upsrt.PAY_2\",\n",
    "\"PAY_3\": \"upsrt.PAY_3\",\n",
    "\"PAY_4\":\"upsrt.PAY_4\",\n",
    "\"PAY_5\": \"upsrt.PAY_5\",\n",
    "\"PAY_6\":\"upsrt.PAY_6\",\n",
    "\"BILL_AMT1\": \"upsrt.BILL_AMT1\",\n",
    "\"BILL_AMT2\": \"upsrt.BILL_AMT2\",\n",
    "\"BILL_AMT3\": \"upsrt.BILL_AMT3\",\n",
    "\"BILL_AMT4\": \"upsrt.BILL_AMT4\",\n",
    "\"BILL_AMT5\": \"upsrt.BILL_AMT5\",\n",
    "\"BILL_AMT6\": \"upsrt.BILL_AMT6\",\n",
    "\"PAY_AMT1\": \"upsrt.PAY_AMT1\"  ,\n",
    "\"PAY_AMT2\": \"upsrt.PAY_AMT2\",\n",
    "\"PAY_AMT3\": \"upsrt.PAY_AMT3\",\n",
    "\"PAY_AMT4\": \"upsrt.PAY_AMT4\",\n",
    "\"PAY_AMT5\": \"upsrt.PAY_AMT5\",\n",
    "\"PAY_AMT6\": \"upsrt.PAY_AMT6\",\n",
    "\"default\": \"upsrt.default\"  }) .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the deltalake table to spark dataframe\n",
    "deltaTable = DeltaTable.forPath(spark, '/opt/spark/python/cc')\n",
    "df_merge_upsert=deltaTable.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_upsert_int=df_merge_upsert.withColumn(\"ID\",col(\"ID\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30011"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_upsert_int.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|ID   |LIMIT_BAL|\n",
      "+-----+---------+\n",
      "|50011|120000   |\n",
      "|50003|50000    |\n",
      "|50002|90000    |\n",
      "|50001|120000   |\n",
      "|50000|10000    |\n",
      "|40000|50000    |\n",
      "|30007|50000    |\n",
      "|30006|50000    |\n",
      "|30005|50000    |\n",
      "|30004|50000    |\n",
      "|30003|50000    |\n",
      "|30000|50000    |\n",
      "|29999|80000    |\n",
      "|29998|30000    |\n",
      "|29997|150000   |\n",
      "|29996|220000   |\n",
      "|29995|80000    |\n",
      "|29994|1e+05    |\n",
      "|29993|10000    |\n",
      "|29992|210000   |\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merge_upsert_int.sort(df_merge_upsert_int.ID.desc()).select(df_merge_upsert_int[\"ID\"],df_merge_upsert_int[\"LIMIT_BAL\"]).show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New row has been added and the existing row is updated\n",
    "1. New row with ID 50011 is added\n",
    "2. The row id with 50000 has the balance of 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullHistoryDF = deltaTable.history() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(fullHistoryDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- version: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- userName: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- operationParameters: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- job: struct (nullable = true)\n",
      " |    |-- jobId: string (nullable = true)\n",
      " |    |-- jobName: string (nullable = true)\n",
      " |    |-- runId: string (nullable = true)\n",
      " |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |-- triggerType: string (nullable = true)\n",
      " |-- notebook: struct (nullable = true)\n",
      " |    |-- notebookId: string (nullable = true)\n",
      " |-- clusterId: string (nullable = true)\n",
      " |-- readVersion: long (nullable = true)\n",
      " |-- isolationLevel: string (nullable = true)\n",
      " |-- isBlindAppend: boolean (nullable = true)\n",
      " |-- operationMetrics: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- userMetadata: string (nullable = true)\n",
      " |-- engineInfo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|timestamp              |operation|version|\n",
      "+-----------------------+---------+-------+\n",
      "|2022-09-29 15:52:32.714|MERGE    |4      |\n",
      "|2022-09-29 15:51:05.804|WRITE    |3      |\n",
      "|2022-09-29 15:49:58.131|DELETE   |2      |\n",
      "|2022-09-29 15:49:56.495|UPDATE   |1      |\n",
      "|2022-09-29 15:49:38.518|WRITE    |0      |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.select(col(\"timestamp\"),col(\"operation\"),col(\"version\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailDF = deltaTable.detail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- format: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- createdAt: timestamp (nullable = true)\n",
      " |-- lastModified: timestamp (nullable = true)\n",
      " |-- partitionColumns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- numFiles: long (nullable = true)\n",
      " |-- sizeInBytes: long (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- minReaderVersion: integer (nullable = true)\n",
      " |-- minWriterVersion: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detailDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+-------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "|format|id                                  |name|description|location                 |createdAt              |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n",
      "+------+------------------------------------+----+-----------+-------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "|delta |0ce09a69-b58c-4da7-8faf-5ece9655d710|null|null       |file:/opt/spark/python/cc|2022-09-29 15:49:35.636|2022-09-29 15:52:32.714|[]              |2       |1770751    |{}        |1               |2               |\n",
      "+------+------------------------------------+----+-----------+-------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detailDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/opt/spark/python/cc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30007"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_travel.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_travel.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Transaction Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tables using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkdataframe> view(saurabh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS saurabh (\n",
    "  id INT,\n",
    "  firstName STRING,\n",
    "  middleName STRING,\n",
    "  lastName STRING,\n",
    "  gender STRING,\n",
    "  birthDate TIMESTAMP,\n",
    "  ssn STRING,\n",
    "  salary INT\n",
    ") USING DELTA\n",
    "\n",
    "''')  \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%SQL` not found.\n"
     ]
    }
   ],
   "source": [
    "%SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS default.people10m (\n",
    "  id INT,\n",
    "  firstName STRING,\n",
    "  middleName STRING,\n",
    "  lastName STRING,\n",
    "  gender STRING,\n",
    "  birthDate TIMESTAMP,\n",
    "  ssn STRING,\n",
    "  salary INT\n",
    ") USING DELTA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
