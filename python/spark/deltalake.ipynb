{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    ".config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and write Delta lake tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the credit card data as spark dataframe\n",
    "df=spark.read.option(\"header\",True).csv(\"/opt/spark/python/mldatasets/UCI_Credit_Card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the credit card data as spark dataframe\n",
    "df=spark.read.option(\"header\",True).csv(\"/mnt/deltalake/df7k3s.ailab.local/delta/dbfs/creditcard/mldatasets/UCI_Credit_Card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the directoty \n",
    "!rm -rf /opt/spark/python/cc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table as delta lake table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/deltalake/df7k3s.ailab.local/delta/dbfs/creditcard/transactionlog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|ID  |\n",
      "+----+\n",
      "|9999|\n",
      "|9998|\n",
      "|9997|\n",
      "|9996|\n",
      "|9995|\n",
      "|9994|\n",
      "|9993|\n",
      "|9992|\n",
      "|9991|\n",
      "|9990|\n",
      "|999 |\n",
      "|9989|\n",
      "|9988|\n",
      "|9987|\n",
      "|9986|\n",
      "|9985|\n",
      "|9984|\n",
      "|9983|\n",
      "|9982|\n",
      "|9981|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.ID.desc()).select(df[\"ID\"]).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file from delta lake\n",
    "df_delta = spark.read.format(\"delta\").load(\"/opt/spark/python/cc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LIMIT_BAL: string (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- MARRIAGE: string (nullable = true)\n",
      " |-- AGE: string (nullable = true)\n",
      " |-- PAY_0: string (nullable = true)\n",
      " |-- PAY_2: string (nullable = true)\n",
      " |-- PAY_3: string (nullable = true)\n",
      " |-- PAY_4: string (nullable = true)\n",
      " |-- PAY_5: string (nullable = true)\n",
      " |-- PAY_6: string (nullable = true)\n",
      " |-- BILL_AMT1: string (nullable = true)\n",
      " |-- BILL_AMT2: string (nullable = true)\n",
      " |-- BILL_AMT3: string (nullable = true)\n",
      " |-- BILL_AMT4: string (nullable = true)\n",
      " |-- BILL_AMT5: string (nullable = true)\n",
      " |-- BILL_AMT6: string (nullable = true)\n",
      " |-- PAY_AMT1: string (nullable = true)\n",
      " |-- PAY_AMT2: string (nullable = true)\n",
      " |-- PAY_AMT3: string (nullable = true)\n",
      " |-- PAY_AMT4: string (nullable = true)\n",
      " |-- PAY_AMT5: string (nullable = true)\n",
      " |-- PAY_AMT6: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column type from string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta=df_delta.withColumn(\"ID\",col(\"ID\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|ID   |\n",
      "+-----+\n",
      "|30007|\n",
      "|30006|\n",
      "|30005|\n",
      "|30004|\n",
      "|30003|\n",
      "|30002|\n",
      "|30001|\n",
      "|30000|\n",
      "|29999|\n",
      "|29998|\n",
      "|29997|\n",
      "|29996|\n",
      "|29995|\n",
      "|29994|\n",
      "|29993|\n",
      "|29992|\n",
      "|29991|\n",
      "|29990|\n",
      "|29989|\n",
      "|29988|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta.sort(df_delta.ID.desc()).select(df_delta[\"ID\"]).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file as delta lake \n",
    "deltaTable = DeltaTable.forPath(spark, \"/opt/spark/python/cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'delta.tables.DeltaTable'>\n"
     ]
    }
   ],
   "source": [
    "print(type(deltaTable))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.update(\n",
    "  condition = col('ID') == '30002',\n",
    "  set = { 'ID': lit(40000) }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete completed\n"
     ]
    }
   ],
   "source": [
    "# Delete 1 row\n",
    "deltaTable=deltaTable.delete(condition = expr(\"ID == 30001\"))\n",
    "# Check the logs \n",
    "print(\"Delete completed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the few rows to the existing delta lake tables\n",
    "df_append=spark.read.option(\"header\",True).csv(\"/opt/spark/python/mldatasets/cc-append.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_append.write.format(\"delta\").mode(\"append\").save(\"/opt/spark/python/cc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert using Merge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge has following features\n",
    "1. Insert new records\n",
    "2. update existing records\n",
    "3. Delete records\n",
    "\n",
    "All above can be done using same statements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column names\n",
    "ID,LIMIT_BAL,SEX,EDUCATION,MARRIAGE,AGE,PAY_0,PAY_2,PAY_3,PAY_4,PAY_5,PAY_6,BILL_AMT1,BILL_AMT2,BILL_AMT3,BILL_AMT4,BILL_AMT5,BILL_AMT6,PAY_AMT1,PAY_AMT2,PAY_AMT3,PAY_AMT4,PAY_AMT5,PAY_AMT6,default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, '/opt/spark/python/cc')\n",
    "df_merge=spark.read.option(\"header\",True).csv(\"/opt/spark/python/mldatasets/cc-upsert.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LIMIT_BAL: string (nullable = true)\n",
      " |-- SEX: string (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- MARRIAGE: string (nullable = true)\n",
      " |-- AGE: string (nullable = true)\n",
      " |-- PAY_0: string (nullable = true)\n",
      " |-- PAY_2: string (nullable = true)\n",
      " |-- PAY_3: string (nullable = true)\n",
      " |-- PAY_4: string (nullable = true)\n",
      " |-- PAY_5: string (nullable = true)\n",
      " |-- PAY_6: string (nullable = true)\n",
      " |-- BILL_AMT1: string (nullable = true)\n",
      " |-- BILL_AMT2: string (nullable = true)\n",
      " |-- BILL_AMT3: string (nullable = true)\n",
      " |-- BILL_AMT4: string (nullable = true)\n",
      " |-- BILL_AMT5: string (nullable = true)\n",
      " |-- BILL_AMT6: string (nullable = true)\n",
      " |-- PAY_AMT1: string (nullable = true)\n",
      " |-- PAY_AMT2: string (nullable = true)\n",
      " |-- PAY_AMT3: string (nullable = true)\n",
      " |-- PAY_AMT4: string (nullable = true)\n",
      " |-- PAY_AMT5: string (nullable = true)\n",
      " |-- PAY_AMT6: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merge.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias('cc') \\\n",
    "  .merge(df_merge.alias('upsrt'),'cc.ID = upsrt.ID') \\\n",
    "  .whenMatchedUpdate(set ={\\\n",
    "\"ID\": \"upsrt.ID\",\n",
    "\"LIMIT_BAL\":\"upsrt.LIMIT_BAL\",\n",
    "\"SEX\": \"upsrt.SEX\",\n",
    "\"EDUCATION\": \"upsrt.EDUCATION\",\n",
    "\"MARRIAGE\": \"upsrt.MARRIAGE\",\n",
    "\"AGE\": \"upsrt.AGE\",\n",
    "\"PAY_0\": \"upsrt.PAY_0\",\n",
    "\"PAY_2\": \"upsrt.PAY_2\",\n",
    "\"PAY_3\": \"upsrt.PAY_3\",\n",
    "\"PAY_4\":\"upsrt.PAY_4\",\n",
    "\"PAY_5\": \"upsrt.PAY_5\",\n",
    "\"PAY_6\":\"upsrt.PAY_6\",\n",
    "\"BILL_AMT1\": \"upsrt.BILL_AMT1\",\n",
    "\"BILL_AMT2\": \"upsrt.BILL_AMT2\",\n",
    "\"BILL_AMT3\": \"upsrt.BILL_AMT3\",\n",
    "\"BILL_AMT4\": \"upsrt.BILL_AMT4\",\n",
    "\"BILL_AMT5\": \"upsrt.BILL_AMT5\",\n",
    "\"BILL_AMT6\": \"upsrt.BILL_AMT6\",\n",
    "\"PAY_AMT1\": \"upsrt.PAY_AMT1\"  ,\n",
    "\"PAY_AMT2\": \"upsrt.PAY_AMT2\",\n",
    "\"PAY_AMT3\": \"upsrt.PAY_AMT3\",\n",
    "\"PAY_AMT4\": \"upsrt.PAY_AMT4\",\n",
    "\"PAY_AMT5\": \"upsrt.PAY_AMT5\",\n",
    "\"PAY_AMT6\": \"upsrt.PAY_AMT6\",\n",
    "\"default\": \"upsrt.default\"  }) \\\n",
    ".whenNotMatchedInsert(values ={ \\\n",
    "\"ID\": \"upsrt.ID\",\n",
    "\"LIMIT_BAL\":\"upsrt.LIMIT_BAL\",\n",
    "\"SEX\": \"upsrt.SEX\",\n",
    "\"EDUCATION\": \"upsrt.EDUCATION\",\n",
    "\"MARRIAGE\": \"upsrt.MARRIAGE\",\n",
    "\"AGE\": \"upsrt.AGE\",\n",
    "\"PAY_0\": \"upsrt.PAY_0\",\n",
    "\"PAY_2\": \"upsrt.PAY_2\",\n",
    "\"PAY_3\": \"upsrt.PAY_3\",\n",
    "\"PAY_4\":\"upsrt.PAY_4\",\n",
    "\"PAY_5\": \"upsrt.PAY_5\",\n",
    "\"PAY_6\":\"upsrt.PAY_6\",\n",
    "\"BILL_AMT1\": \"upsrt.BILL_AMT1\",\n",
    "\"BILL_AMT2\": \"upsrt.BILL_AMT2\",\n",
    "\"BILL_AMT3\": \"upsrt.BILL_AMT3\",\n",
    "\"BILL_AMT4\": \"upsrt.BILL_AMT4\",\n",
    "\"BILL_AMT5\": \"upsrt.BILL_AMT5\",\n",
    "\"BILL_AMT6\": \"upsrt.BILL_AMT6\",\n",
    "\"PAY_AMT1\": \"upsrt.PAY_AMT1\"  ,\n",
    "\"PAY_AMT2\": \"upsrt.PAY_AMT2\",\n",
    "\"PAY_AMT3\": \"upsrt.PAY_AMT3\",\n",
    "\"PAY_AMT4\": \"upsrt.PAY_AMT4\",\n",
    "\"PAY_AMT5\": \"upsrt.PAY_AMT5\",\n",
    "\"PAY_AMT6\": \"upsrt.PAY_AMT6\",\n",
    "\"default\": \"upsrt.default\"  }) .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the deltalake table to spark dataframe\n",
    "deltaTable = DeltaTable.forPath(spark, '/opt/spark/python/cc')\n",
    "df_merge_upsert=deltaTable.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_upsert_int=df_merge_upsert.withColumn(\"ID\",col(\"ID\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30011"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_upsert_int.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|ID   |LIMIT_BAL|\n",
      "+-----+---------+\n",
      "|50011|120000   |\n",
      "|50003|50000    |\n",
      "|50002|90000    |\n",
      "|50001|120000   |\n",
      "|50000|10000    |\n",
      "|40000|50000    |\n",
      "|30007|50000    |\n",
      "|30006|50000    |\n",
      "|30005|50000    |\n",
      "|30004|50000    |\n",
      "|30003|50000    |\n",
      "|30000|50000    |\n",
      "|29999|80000    |\n",
      "|29998|30000    |\n",
      "|29997|150000   |\n",
      "|29996|220000   |\n",
      "|29995|80000    |\n",
      "|29994|1e+05    |\n",
      "|29993|10000    |\n",
      "|29992|210000   |\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merge_upsert_int.sort(df_merge_upsert_int.ID.desc()).select(df_merge_upsert_int[\"ID\"],df_merge_upsert_int[\"LIMIT_BAL\"]).show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New row has been added and the existing row is updated\n",
    "1. New row with ID 50011 is added\n",
    "2. The row id with 50000 has the balance of 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullHistoryDF = deltaTable.history() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(fullHistoryDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- version: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- userName: string (nullable = true)\n",
      " |-- operation: string (nullable = true)\n",
      " |-- operationParameters: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- job: struct (nullable = true)\n",
      " |    |-- jobId: string (nullable = true)\n",
      " |    |-- jobName: string (nullable = true)\n",
      " |    |-- runId: string (nullable = true)\n",
      " |    |-- jobOwnerId: string (nullable = true)\n",
      " |    |-- triggerType: string (nullable = true)\n",
      " |-- notebook: struct (nullable = true)\n",
      " |    |-- notebookId: string (nullable = true)\n",
      " |-- clusterId: string (nullable = true)\n",
      " |-- readVersion: long (nullable = true)\n",
      " |-- isolationLevel: string (nullable = true)\n",
      " |-- isBlindAppend: boolean (nullable = true)\n",
      " |-- operationMetrics: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- userMetadata: string (nullable = true)\n",
      " |-- engineInfo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+-------+\n",
      "|timestamp              |operation|version|\n",
      "+-----------------------+---------+-------+\n",
      "|2022-09-29 15:52:32.714|MERGE    |4      |\n",
      "|2022-09-29 15:51:05.804|WRITE    |3      |\n",
      "|2022-09-29 15:49:58.131|DELETE   |2      |\n",
      "|2022-09-29 15:49:56.495|UPDATE   |1      |\n",
      "|2022-09-29 15:49:38.518|WRITE    |0      |\n",
      "+-----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullHistoryDF.select(col(\"timestamp\"),col(\"operation\"),col(\"version\")).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailDF = deltaTable.detail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- format: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- createdAt: timestamp (nullable = true)\n",
      " |-- lastModified: timestamp (nullable = true)\n",
      " |-- partitionColumns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- numFiles: long (nullable = true)\n",
      " |-- sizeInBytes: long (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- minReaderVersion: integer (nullable = true)\n",
      " |-- minWriterVersion: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detailDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+-------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "|format|id                                  |name|description|location                 |createdAt              |lastModified           |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n",
      "+------+------------------------------------+----+-----------+-------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "|delta |0ce09a69-b58c-4da7-8faf-5ece9655d710|null|null       |file:/opt/spark/python/cc|2022-09-29 15:49:35.636|2022-09-29 15:52:32.714|[]              |2       |1770751    |{}        |1               |2               |\n",
      "+------+------------------------------------+----+-----------+-------------------------+-----------------------+-----------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detailDF.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"/opt/spark/python/cc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30007"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_travel.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_travel.s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Transaction Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create tables using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysparkdataframe> view(saurabh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS saurabh (\n",
    "  id INT,\n",
    "  firstName STRING,\n",
    "  middleName STRING,\n",
    "  lastName STRING,\n",
    "  gender STRING,\n",
    "  birthDate TIMESTAMP,\n",
    "  ssn STRING,\n",
    "  salary INT\n",
    ") USING DELTA\n",
    "\n",
    "''')  \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%SQL` not found.\n"
     ]
    }
   ],
   "source": [
    "%SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS default.people10m (\n",
    "  id INT,\n",
    "  firstName STRING,\n",
    "  middleName STRING,\n",
    "  lastName STRING,\n",
    "  gender STRING,\n",
    "  birthDate TIMESTAMP,\n",
    "  ssn STRING,\n",
    "  salary INT\n",
    ") USING DELTA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
